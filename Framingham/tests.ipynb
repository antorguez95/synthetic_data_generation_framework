{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Dependencies \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import time \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, KMeansSMOTE, SVMSMOTE, BorderlineSMOTE\n",
    "\n",
    "from Framingham_utils import *\n",
    "\n",
    "from exploratory_data_analysys import *\n",
    "\n",
    "from sdg_utils import * \n",
    "\n",
    "from sdv import Metadata\n",
    "from sdv.tabular import GaussianCopula\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from train_utils import *\n",
    "\n",
    "from model_evaluation import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save working directory to return to it \n",
    "wd = os.getcwd()\n",
    "print(wd)\n",
    "# Declare dataset path and get into it\n",
    "DATASET_PATH = r\"C:\\Users\\aralmeida\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Doctorado\\Bases de datos\\Diabetes\\Framingham\"\n",
    "\n",
    "# File name \n",
    "filename = \"framingham_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Dependencies \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import time \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import sys \n",
    "sys.path.append('..')\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, SMOTENC, KMeansSMOTE, SVMSMOTE, BorderlineSMOTE\n",
    "\n",
    "from Framingham_utils import *\n",
    "\n",
    "from exploratory_data_analysys import *\n",
    "\n",
    "from sdg_utils import * \n",
    "\n",
    "from sdv import Metadata\n",
    "from sdv.tabular import GaussianCopula\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from train_utils import *\n",
    "\n",
    "from model_evaluation import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save working directory to return to it \n",
    "wd = os.getcwd()\n",
    "print(wd)\n",
    "# Declare dataset path and get into it\n",
    "DATASET_PATH = r\"C:\\Users\\aralmeida\\OneDrive - Universidad de Las Palmas de Gran Canaria\\Doctorado\\Bases de datos\\Diabetes\\Framingham\"\n",
    "\n",
    "# File name \n",
    "filename = \"framingham_data.csv\"\n",
    "\n",
    "# Monitoring computational time \n",
    "start = time.time()\n",
    "\n",
    "# Prepare Alzheimer-Balea database to be handled\n",
    "data, X, Y, feat_names, y_tag = prepare_Framingham(dataset_path = DATASET_PATH, filename = filename)\n",
    "\n",
    "# Return from dataset directory to working directory \n",
    "os.chdir(wd)\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "eda(data, X, Y, 'Framingham')\n",
    "\n",
    "# Data partition - Train (80%) + Validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=4)\n",
    "print ('Training set:', X_train.shape,  y_train.shape)\n",
    "print ('Validation set:', X_val.shape,  y_val.shape)\n",
    "\n",
    "# Concatenate X and Y dataframes \n",
    "train_data = pd.concat([X_train, y_train], axis = 1)\n",
    "validation_data = pd.concat([X_val, y_val], axis = 1)\n",
    "\n",
    "# KNN-Imputation (training and validation set separately)\n",
    "# Imputer declaration \n",
    "imputer = KNNImputer(missing_values = np.nan , n_neighbors = 5, weights = 'uniform', metric = 'nan_euclidean',\n",
    "                      copy = 'false') \n",
    "\n",
    "# Imputation (training and validation separately)\n",
    "train_data = imputer.fit_transform(train_data)\n",
    "validation_data = imputer.fit_transform(validation_data) \n",
    "\n",
    "# Conversion from np.array to pd.DataFrame and convert from float to original datatype\n",
    "train_data, X_train, y_train = numerical_conversion(train_data, feat_names, y_tag)\n",
    "validation_data, X_val, y_val = numerical_conversion(validation_data, feat_names, y_tag)\n",
    "\n",
    "# Separate control and cases to duplicate together and separated\n",
    "controls = train_data.loc[(train_data[y_tag]==0)] \n",
    "cases = train_data.loc[(train_data[y_tag]==1)]\n",
    "\n",
    "# Calculate cases/control ratio\n",
    "train_ratio = (train_data[y_tag][train_data[y_tag] == 1].value_counts()[1])/(train_data[y_tag][train_data[y_tag] == 0].value_counts()[0])\n",
    "\n",
    "# Exploratory Data Analysis after KNN Imputation \n",
    "# eda(train_data, X_train, y_train, 'PIMA', folder = r\"./EDA_train\")\n",
    "# eda(validation_data, X_val, y_val, 'PIMA', folder = r\"./EDA_val\")\n",
    "\n",
    "# Data augmentation models: Balancing and Augmentation steps \n",
    "# a) Balancing data \n",
    "# This piece of code is for estimating which balancing models better work on this database. \n",
    "# Flag \"CHECKED\" should be set to True after evaluation has to skip the balancing evaluation\n",
    "# part once this step was performed \n",
    "\n",
    "# Path to store obtained reusults\n",
    "STORE_PATH = r\".\\results\"\n",
    "\n",
    "# Flag \n",
    "BALANCING_CHECKED = False\n",
    "\n",
    "# Categorical features indexes\n",
    "cat_feat_idxs = [2]\n",
    "\n",
    "# Set number of iterations to be done for the balancing evaluation \n",
    "iterations = 100\n",
    "\n",
    "# Balancing algorithms evaluation\n",
    "if BALANCING_CHECKED  == False : \n",
    "    print(\"Balancing algorithms evaluation... %i iterations running\" % iterations)\n",
    "    balancing_eval('Framingham', X_train, y_train, train_data,\n",
    "                 feat_names, y_tag, [\"ADASYN\", \"SMOTE\", \"SMOTENC\", \"KMeansSmote\", \"SVMSMOTE\", \"BorderlineSMOTE\"], \n",
    "                  cat_feat_idxs, filename = \"balancing_metrics.csv\" , iterations = iterations, store_path = STORE_PATH)\n",
    "\n",
    "# In case balancing evaluation is not done, change directory\n",
    "else: \n",
    "    os.chdir(STORE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strings of selected methods (after analaysing previous step results (i.e., balancing_eval())\n",
    "balance1 = \"ADASYN\"\n",
    "balance2 = \"NC\"\n",
    "\n",
    "# Balancing with chosen method I \n",
    "X_balance1, y_balance1 = ADASYN(sampling_strategy = 'minority',\n",
    "                                                random_state = None,\n",
    "                                                n_neighbors = 5,\n",
    "                                                n_jobs = None).fit_resample(X_train, y_train) \n",
    "\n",
    "# Balancing with chosen method II \n",
    "X_balance2, y_balance2 = SMOTENC(categorical_features = cat_feat_idxs,\n",
    "                                                sampling_strategy = 'minority', \n",
    "                                                random_state = None,\n",
    "                                                k_neighbors = 5,\n",
    "                                                n_jobs = None).fit_resample(X_train, y_train)\n",
    "\n",
    "# Add column Y  to dataframe\n",
    "# ADASYN \n",
    "X_balance1.reset_index(drop=True, inplace=True)\n",
    "y_balance1.reset_index(drop=True, inplace=True)\n",
    "data_balance1= pd.concat([X_balance1, y_balance1], axis = 1)\n",
    "\n",
    "# SMOTE-NC\n",
    "X_balance2.reset_index(drop=True, inplace=True)\n",
    "y_balance2.reset_index(drop=True, inplace=True)\n",
    "data_balance2 = pd.concat([X_balance2, y_balance2], axis = 1)\n",
    "\n",
    "# Data type conversion and control/cases splitting to generate samples separately\n",
    "# Training set \n",
    "train_data = general_conversion(train_data)\n",
    "controls = train_data.loc[(train_data[y_tag]==0)] \n",
    "cases = train_data.loc[(train_data[y_tag]==1)]\n",
    "\n",
    "# Balance 1\n",
    "data_balance1 = general_conversion(data_balance1) \n",
    "controls_balance1 = data_balance1.loc[(data_balance1[y_tag]==0)] \n",
    "cases_balance1 = data_balance1.loc[(data_balance1[y_tag]==1)] \n",
    "\n",
    "# Balance 2\n",
    "data_balance2 = general_conversion(data_balance2)\n",
    "controls_balance2 = data_balance2.loc[(data_balance2[y_tag]==0)] \n",
    "cases_balance2 = data_balance2.loc[(data_balance2[y_tag]==1)]\n",
    "\n",
    "# Validation (non-splitted)\n",
    "validation_data = general_conversion(validation_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Defining metadata for Early-DM database\n",
    "metadata = Metadata()\n",
    "metadata.add_table(\n",
    "    name = 'EarlyDM',\n",
    "    data = train_data)\n",
    "    #fields_metadata = earlyDM_fields)\n",
    "\n",
    "# A) Gaussian Copula model\n",
    "gc = GaussianCopula(field_types = framingham_fields, \n",
    "                    #constraints = constraints, \n",
    "                    field_distributions = framingham_distributions)\n",
    " \n",
    "# B) CTGAN model \n",
    "from sdv.tabular import CTGAN\n",
    "ctgan = CTGAN(field_types = framingham_fields,\n",
    "              cuda = True)\n",
    "\n",
    "# Set conditions for synthetic generation \n",
    "cond_positive = {\n",
    "    'TenYearCHD': 1\n",
    "    }\n",
    "cond_negative = {\n",
    "    'TenYearCHD': 0\n",
    "    }\n",
    "\n",
    "# Strings to generate files depending on the used synthetic data generation model \n",
    "augmen1 = \"CTGAN\"\n",
    "augmen2 = \"GC\"\n",
    "\n",
    "# Number of iterations in the process of data augmentation \n",
    "iterations = 1\n",
    "\n",
    "# Differentiating between categorical and numerical features. The former are \n",
    "# one-hot encoded and the latter are standardized\n",
    "categorical_features = ['education']\n",
    "numerical_features = [ 'age','cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
    "\n",
    "# Machine Learning models and hyperparameters declaration \n",
    "# SVM \n",
    "svm_params = {\"kernel\" : ['rbf', 'linear'],\n",
    "              \"C\" : [0.1, 1, 2.5, 5, 10],\n",
    "              \"gamma\" : [0.01, 0.1, 1, 10],\n",
    "              }\n",
    "svm_params = {\n",
    "              \"C\" : [0.1],\n",
    "              }\n",
    "svm_model = SVC(random_state = 12345, cache_size=200, max_iter = -1, probability=True)\n",
    "\n",
    "# RF\n",
    "rf_params = {\"n_estimators\": [20, 50, 100, 200], \n",
    "              \"max_features\": [2,3,5,7],\n",
    "              } \n",
    "rf_params = {\"n_estimators\": [20],\n",
    "              }  \n",
    "rf_model = RandomForestClassifier(random_state = 12345)\n",
    "\n",
    "# XGB \n",
    "xgb_params = {\"learning_rate\": [0.01, 0.1, 0.5],\n",
    "              \"n_estimators\": [20]\n",
    "              }\n",
    "xgb_params = {\n",
    "              \"n_estimators\": [20]\n",
    "              }\n",
    "xgb_model = GradientBoostingClassifier(random_state = 12345)\n",
    "\n",
    "# KNN\n",
    "knn_params = {\"n_neighbors\": [6,8,10,12,14,16],\n",
    "              \"weights\" : ['uniform','distance'],\n",
    "              }\n",
    "knn_params = {\"n_neighbors\": [6],\n",
    "              }\n",
    "knn_model = KNeighborsClassifier(algorithm = 'auto', n_jobs = -1)\n",
    "\n",
    "# Based on the original training data size, different amount of synthetic data is generated\n",
    "sizes_balance1 = [round(data_balance1.shape[0]/4), round(data_balance1.shape[0]/2), round(data_balance1.shape[0]), \n",
    "         round(data_balance1.shape[0]*2), round(data_balance1.shape[0]*4), round(data_balance1.shape[0]*4)+20]\n",
    "sizes_balance2 = [round(data_balance2.shape[0]/4), round(data_balance2.shape[0]/2), round(data_balance2.shape[0]), \n",
    "         round(data_balance2.shape[0]*2), round(data_balance2.shape[0]*4), round(data_balance2.shape[0]*4)+20]\n",
    "sizes_only_augmen1 = [round(X_train.shape[0]/4), round(X_train.shape[0]/2), round(X_train.shape[0]), \n",
    "         round(X_train.shape[0]*2), round(X_train.shape[0]*4), round(data_balance2.shape[0]*4)+20]\n",
    "\n",
    "\n",
    "sizes_balance1 = [100, 200, 400, 800, 1000, 1020]\n",
    "sizes_balance2 = [100, 200, 400, 800, 1000, 1020]\n",
    "sizes_only_augmen1 = [100, 200, 400, 800, 1000, 1020]\n",
    "\n",
    "\n",
    "# List with the string keys to handle above values \n",
    "sizes_keys = [\"quarter\", \"half\", \"unit\", \"double\", \"quadruple\", \"only-synth\"]\n",
    "\n",
    "# Strings containing combinations of SDG (Synthetic Data Generators) \n",
    "comb1 = (\"%s + %s\") % (balance1, augmen1)\n",
    "comb2 = (\"%s + %s\") % (balance1, augmen2)\n",
    "comb3 = (\"%s + %s\") % (balance2, augmen1)\n",
    "comb4 = (\"%s + %s\") % (balance2, augmen2)\n",
    "comb5 = (\"%s + Sep. + %s\") % (balance1, augmen1)\n",
    "comb6 = (\"%s + Sep. + %s\") % (balance1, augmen2)\n",
    "comb7 = (\"%s + Sep. + %s\") % (balance2, augmen1)\n",
    "comb8 = (\"%s + Sep. + %s\") % (balance2, augmen2)\n",
    "comb9 = \"%s\" % (augmen1)\n",
    "comb10 = \"Sep. + %s\" % (augmen1)\n",
    "\n",
    "# Methods put alltogether to create dictionary keys \n",
    "sdg_combinations = [comb1, comb2, comb3, comb4, comb5,\n",
    "           comb6, comb7, comb8, comb9, comb10]\n",
    "\n",
    "class_metrics = ['acc', 'auc', 'f1']\n",
    "\n",
    "sdg_metrics, class_metrics, hyperparameters = get_eval_dictionaries(sdg_combinations, sizes_keys, class_metrics, iterations)\n",
    "\n",
    "# List of tuples containing combinations of balancing, augmenting methods and their associated strings\n",
    "# Non-splitting and no conditions tuples\n",
    "no_split = [(augmen1, balance1, data_balance1, sizes_balance1),\n",
    "(augmen1, balance2, data_balance2, sizes_balance2),\n",
    "(augmen2, balance1, data_balance1, sizes_balance1),\n",
    "(augmen2, balance2, data_balance2, sizes_balance1)]\n",
    "\n",
    "# Splitting with no conditions tuples\n",
    "split = [(augmen1, balance1, controls_balance1, cases_balance1, sizes_balance1),\n",
    "(augmen1, balance2, controls_balance2, cases_balance2, sizes_balance2),\n",
    "(augmen2, balance1, controls_balance1, cases_balance1, sizes_balance1),\n",
    "(augmen2, balance2, controls_balance2, cases_balance2, sizes_balance2)]\n",
    "\n",
    "# Replacement of numbers by categories to make one-hot encoding aferwards (trainig and validation set)\n",
    "train_data = num2cat(train_data)\n",
    "train_data = one_hot_enc(train_data)\n",
    "\n",
    "cols_names = train_data.columns[0:len(train_data.columns)-1]\n",
    "X_train = train_data[cols_names]\n",
    "validation_data = num2cat(validation_data)\n",
    "validation_data = one_hot_enc(validation_data)\n",
    "X_val = validation_data.drop([y_tag], axis=1)\n",
    "y_val = validation_data[y_tag]\n",
    "\n",
    "# Computes training set reference metrics just once to compare it to the different synthetic methods\n",
    "train_PMFs, train_hist_bases = PMF(X_train)\n",
    "\n",
    "# Standardization of Training and Validation set to train and validate\n",
    "X_train_norm, y_train_norm = standardization_cat(X_train, y_train, numerical_features)\n",
    "X_val_norm, y_val_norm = standardization_cat(X_val, y_val, numerical_features)\n",
    "\n",
    "# Control/Cases ratio for ADASYN and Borderline \n",
    "balance1_ratio = (data_balance1[y_tag][data_balance1[y_tag] == 1].value_counts()[1])/(data_balance1[y_tag][data_balance1[y_tag] == 0].value_counts()[0])\n",
    "balance2_ratio = (data_balance2[y_tag][data_balance2[y_tag] == 1].value_counts()[1])/(data_balance2[y_tag][data_balance2[y_tag] == 0].value_counts()[0])\n",
    "\n",
    "# Models' training with the original training sets without synthetic samples \n",
    "SVM_model, SVM_train_results, SVM_cv_results  = model_train(svm_model, svm_params, \"SVM\", \"No synthetic\", X_train_norm, y_train_norm, cv = 10, scoring = 'f1')\n",
    "rf_model, rf_train_results, rf_cv_results  = model_train(rf_model, rf_params, \"RF\", \"No synthetic\", X_train_norm, y_train_norm, cv = 10, scoring = 'f1')\n",
    "xgb_model, xgb_train_results, xgb_cv_results  = model_train(xgb_model, xgb_params, \"SVM\", \"No synthetic\", X_train_norm, y_train_norm, cv = 10, scoring = 'f1')\n",
    "knn_model, knn_train_results, knn_cv_results  = model_train(knn_model, knn_params, \"SVM\", \"No synthetic\", X_train_norm, y_train_norm, cv = 10, scoring = 'f1')\n",
    "\n",
    "# Models' evaluation with the original training sets without synthetic samples\n",
    "SVM_acc_nosynth, SVM_auc_nosynth, SVM_f1_nosynth = acc_auc_roc_SVM(SVM_model, X_val_norm, y_val_norm)\n",
    "rf_acc_nosynth, rf_auc_nosynth, rf_f1_nosynth = acc_auc_roc_general(rf_model, X_val_norm, y_val_norm)\n",
    "xgb_acc_nosynth, xgb_auc_nosynth, xgb_f1_nosynth = acc_auc_roc_general(xgb_model, X_val_norm, y_val_norm)\n",
    "knn_acc_nosynth, knn_auc_nosynth, knn_f1_nosynth = acc_auc_roc_general(knn_model, X_val_norm, y_val_norm)\n",
    "\n",
    "# Save F1-score values for further use (load_and_plot.py)\n",
    "with open(\"svm_f1.txt\", \"wb\") as svm:\n",
    "    pickle.dump(SVM_f1_nosynth, svm)\n",
    "svm.close()\n",
    "with open(\"rf_f1.txt\", \"wb\") as rf:\n",
    "    pickle.dump(rf_f1_nosynth, rf)\n",
    "rf.close()\n",
    "with open(\"xgb_f1.txt\", \"wb\") as xgb:\n",
    "    pickle.dump(xgb_f1_nosynth, xgb)\n",
    "xgb.close()\n",
    "with open(\"knn_f1.txt\", \"wb\") as knn:\n",
    "    pickle.dump(SVM_f1_nosynth, knn)\n",
    "knn.close()\n",
    "\n",
    "# Save \"sizes_balance1\" variables for further use (load_and_plot.py)\n",
    "with open(\"sizes.txt\", \"wb\") as siz:\n",
    "    pickle.dump(sizes_balance1, siz)\n",
    "siz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates all the previously indicated number of synthetic data samples \n",
    "for i in range(len(sizes_keys)):\n",
    "\n",
    "    print(\"##################################### CHIQUILLOS ES EL TAMAÑO %i ############\" % i)\n",
    "\n",
    "    # Different iterations to evaluate variability of iterations \n",
    "    for j in range(iterations):\n",
    "          \n",
    "        # Data augmentation WITHOUT splitting between controls and cases \n",
    "        for group in no_split : \n",
    "\n",
    "            # Synthetic data generation with CTGAN and Gaussian Copula \n",
    "            if group[0] == augmen2 : \n",
    "                mixed_data, synthetic_data = data_aug(gc, group, i, j)\n",
    "            elif group[0] == augmen1 :\n",
    "                mixed_data, synthetic_data = data_aug(ctgan, group, i, j) \n",
    "\n",
    "            # The case where only synthetic data is used to train the model\n",
    "            if sizes_keys[i] == \"only-synth\":\n",
    "                mixed_data = synthetic_data\n",
    "            else : \n",
    "                mixed_data = mixed_data \n",
    "            \n",
    "            # Performs replacement and one-hot enconding in the training set without synthetic samples\n",
    "            mixed_data = num2cat(mixed_data)\n",
    "            mixed_data = one_hot_enc(mixed_data)\n",
    "            \n",
    "            # Saving the key combining balancing and augmentation technique strings\n",
    "            method = group[1] + \" + \" + group[0] \n",
    "\n",
    "            # Compute metrics \n",
    "            PCD_val = PCD(mixed_data, train_data)\n",
    "            KLD_val, _, _ = KLD(mixed_data.loc[:, mixed_data.columns != y_tag], train_PMFs, train_hist_bases)\n",
    "            MMD_val = mmd_linear(X_train.to_numpy(), mixed_data.loc[:, mixed_data.columns != y_tag].to_numpy())\n",
    "            ratio = (mixed_data[y_tag][mixed_data[y_tag] == 1].value_counts()[1])/(mixed_data[y_tag][mixed_data[y_tag] == 0].value_counts()[0])\n",
    "            \n",
    "            # Store metrics in dictionary \n",
    "            sdg_metrics[method][sizes_keys[i]]['PCD'][j] = PCD_val \n",
    "            sdg_metrics[method][sizes_keys[i]]['KLD'][j] = KLD_val \n",
    "            sdg_metrics[method][sizes_keys[i]]['MMD'][j] = MMD_val \n",
    "            sdg_metrics[method][sizes_keys[i]]['ratio'][j] = ratio \n",
    "            \n",
    "            # Standardization\n",
    "            X_norm, y_norm = standardization_cat(mixed_data.loc[:, mixed_data.columns != y_tag], mixed_data[y_tag], numerical_features)\n",
    "\n",
    "            # ML models train \n",
    "            SVM_model, SVM_train_results, SVM_cv_results  = model_train(svm_model, svm_params, \"SVM\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            rf_model, rf_train_results, rf_cv_results = model_train(rf_model, rf_params, \"RF\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            xgb_model, xgb_train_results, xgb_cv_results = model_train(xgb_model, xgb_params, \"XGB\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            knn_model, knn_train_results, knn_cv_results = model_train(knn_model, knn_params, \"KNN\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            \n",
    "            # Saving hyperparameters in the corresponding dictionary \n",
    "            hyperparameters['SVM'][method][sizes_keys[i]]['kernel'][j] = SVM_model.kernel\n",
    "            hyperparameters['SVM'][method][sizes_keys[i]]['C'][j] = SVM_model.C\n",
    "            hyperparameters['SVM'][method][sizes_keys[i]]['gamma'][j] = SVM_model.gamma\n",
    "\n",
    "            hyperparameters['RF'][method][sizes_keys[i]]['estimators'][j] = rf_model.n_estimators\n",
    "            hyperparameters['RF'][method][sizes_keys[i]]['max_feat'][j] = rf_model.max_features\n",
    "            \n",
    "            hyperparameters['XGB'][method][sizes_keys[i]]['estimators'][j] = xgb_model.n_estimators\n",
    "            hyperparameters['XGB'][method][sizes_keys[i]]['lr'][j] = xgb_model.learning_rate\n",
    "\n",
    "            hyperparameters['KNN'][method][sizes_keys[i]]['neigh'][j] = knn_model.n_neighbors\n",
    "            hyperparameters['KNN'][method][sizes_keys[i]]['C'][j] = knn_model.weights\n",
    "\n",
    "            # ML models' evaluation \n",
    "            SVM_acc, SVM_auc, SVM_f1 = acc_auc_roc_SVM(SVM_model, X_val_norm, y_val_norm)\n",
    "            rf_acc, rf_auc, rf_f1 = acc_auc_roc_general(rf_model, X_val_norm, y_val_norm)\n",
    "            xgb_acc, xgb_auc, xgb_f1 = acc_auc_roc_general(xgb_model, X_val_norm, y_val_norm)\n",
    "            knn_acc, knn_auc, knn_f1 = acc_auc_roc_general(knn_model, X_val_norm, y_val_norm)\n",
    "            \n",
    "            # Store results in correspondant dictionary\n",
    "            class_metrics['SVM'][method][sizes_keys[i]]['acc'][j] = SVM_acc\n",
    "            class_metrics['SVM'][method][sizes_keys[i]]['auc'][j] = SVM_auc\n",
    "            class_metrics['SVM'][method][sizes_keys[i]]['f1'][j] = SVM_f1\n",
    "\n",
    "            class_metrics['RF'][method][sizes_keys[i]]['acc'][j]= rf_acc\n",
    "            class_metrics['RF'][method][sizes_keys[i]]['auc'][j] = rf_auc\n",
    "            class_metrics['RF'][method][sizes_keys[i]]['f1'][j] = rf_f1\n",
    "\n",
    "            class_metrics['XGB'][method][sizes_keys[i]]['acc'][j] = xgb_acc\n",
    "            class_metrics['XGB'][method][sizes_keys[i]]['auc'][j] = xgb_auc\n",
    "            class_metrics['XGB'][method][sizes_keys[i]]['f1'] = xgb_f1\n",
    "\n",
    "            class_metrics['KNN'][method][sizes_keys[i]]['acc'][j] = knn_acc\n",
    "            class_metrics['KNN'][method][sizes_keys[i]]['auc'][j] = knn_auc\n",
    "            class_metrics['KNN'][method][sizes_keys[i]]['f1'][j] = knn_f1\n",
    "        \n",
    "        # Data augmentation AFTER SPLITTING bewteen controls and cases  \n",
    "        #### NOTE that CONDITIONS are only applied in CTGAN augmentation\n",
    "        # due to its poor performance without fixing them, whereas Gaussian Copula\n",
    "        # generates synthetic control or cases samples from the control and cases\n",
    "        # datasets, respectively, without the need of fixing the output variable. \n",
    "\n",
    "        for group in split :\n",
    "        \n",
    "            # Synthetic data generation with CTGAN and Gaussian Copula \n",
    "            if group[0] == augmen2 : \n",
    "                mixed_data, synthetic_data = data_aug_after_split(gc, group, i, j)\n",
    "            elif group[0] == augmen1 :\n",
    "                mixed_data, synthetic_data = data_aug_cond_after_split(ctgan, group, [cond_negative, cond_positive], i, j)  \n",
    "            \n",
    "            # The case where only synthetic data is used to train the model\n",
    "            if sizes_keys[i] == \"only-synth\":\n",
    "                mixed_data = synthetic_data\n",
    "            else : \n",
    "                mixed_data = mixed_data \n",
    "            \n",
    "            # Performs replacement and one-hot enconding in the training set without synthetic samples\n",
    "            mixed_data = num2cat(mixed_data)\n",
    "            mixed_data = one_hot_enc(mixed_data)\n",
    "            \n",
    "            # Saving the key combining balancing and augmentation technique strings\n",
    "            method = group[1] + \" + Sep. + \" + group[0] \n",
    "            \n",
    "            # Compute metrics \n",
    "            PCD_val = PCD(mixed_data, train_data)\n",
    "            KLD_val, _, _ = KLD(mixed_data.loc[:, mixed_data.columns != y_tag], train_PMFs, train_hist_bases)\n",
    "            MMD_val = mmd_linear(X_train.to_numpy(), mixed_data.loc[:, mixed_data.columns != y_tag].to_numpy())\n",
    "            ratio = (mixed_data[y_tag][mixed_data[y_tag] == 1].value_counts()[1])/(mixed_data[y_tag][mixed_data[y_tag] == 0].value_counts()[0])\n",
    "            \n",
    "            # Store metrics in dictionary \n",
    "            sdg_metrics[method][sizes_keys[i]]['PCD'][j] = PCD_val \n",
    "            sdg_metrics[method][sizes_keys[i]]['KLD'][j] = KLD_val \n",
    "            sdg_metrics[method][sizes_keys[i]]['MMD'][j] = MMD_val \n",
    "            sdg_metrics[method][sizes_keys[i]]['ratio'][j] = ratio \n",
    "            \n",
    "            # Standardization\n",
    "            X_norm, y_norm = standardization(mixed_data.loc[:, mixed_data.columns != y_tag], mixed_data[y_tag])\n",
    "\n",
    "            # ML models train \n",
    "            SVM_model, SVM_train_results, SVM_cv_results  = model_train(svm_model, svm_params, \"SVM\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            rf_model, rf_train_results, rf_cv_results = model_train(rf_model, rf_params, \"RF\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            xgb_model, xgb_train_results, xgb_cv_results = model_train(xgb_model, xgb_params, \"XGB\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            knn_model, knn_train_results, knn_cv_results = model_train(knn_model, knn_params, \"KNN\", method, X_norm, y_norm, cv = 10, scoring = 'f1')\n",
    "            \n",
    "            # Saving hyperparameters in the corresponding dictionary \n",
    "            hyperparameters['SVM'][method][sizes_keys[i]]['kernel'][j] = SVM_model.kernel\n",
    "            hyperparameters['SVM'][method][sizes_keys[i]]['C'][j] = SVM_model.C\n",
    "            hyperparameters['SVM'][method][sizes_keys[i]]['gamma'][j] = SVM_model.gamma\n",
    "\n",
    "            hyperparameters['RF'][method][sizes_keys[i]]['estimators'][j] = rf_model.n_estimators\n",
    "            hyperparameters['RF'][method][sizes_keys[i]]['max_feat'][j] = rf_model.max_features\n",
    "            \n",
    "            hyperparameters['XGB'][method][sizes_keys[i]]['estimators'][j] = xgb_model.n_estimators\n",
    "            hyperparameters['XGB'][method][sizes_keys[i]]['lr'][j] = xgb_model.learning_rate\n",
    "\n",
    "            hyperparameters['KNN'][method][sizes_keys[i]]['neigh'][j] = knn_model.n_neighbors\n",
    "            hyperparameters['KNN'][method][sizes_keys[i]]['C'][j] = knn_model.weights\n",
    "\n",
    "            # ML models' evaluation \n",
    "            SVM_acc, SVM_auc, SVM_f1 = acc_auc_roc_SVM(SVM_model, X_val_norm, y_val_norm)\n",
    "            rf_acc, rf_auc, rf_f1 = acc_auc_roc_general(rf_model, X_val_norm, y_val_norm)\n",
    "            xgb_acc, xgb_auc, xgb_f1 = acc_auc_roc_general(xgb_model, X_val_norm, y_val_norm)\n",
    "            knn_acc, knn_auc, knn_f1 = acc_auc_roc_general(knn_model, X_val_norm, y_val_norm)\n",
    "            \n",
    "            # Store results in correspondant dictionary\n",
    "            class_metrics['SVM'][method][sizes_keys[i]]['acc'][j] = SVM_acc\n",
    "            class_metrics['SVM'][method][sizes_keys[i]]['auc'][j] = SVM_auc\n",
    "            class_metrics['SVM'][method][sizes_keys[i]]['f1'][j] = SVM_f1\n",
    "\n",
    "            class_metrics['RF'][method][sizes_keys[i]]['acc'][j] = rf_acc\n",
    "            class_metrics['RF'][method][sizes_keys[i]]['auc'][j] = rf_auc\n",
    "            class_metrics['RF'][method][sizes_keys[i]]['f1'][j] = rf_f1\n",
    "\n",
    "            class_metrics['XGB'][method][sizes_keys[i]]['acc'][j] = xgb_acc\n",
    "            class_metrics['XGB'][method][sizes_keys[i]]['auc'][j] = xgb_auc\n",
    "            class_metrics['XGB'][method][sizes_keys[i]]['f1'][j] = xgb_f1\n",
    "\n",
    "            class_metrics['KNN'][method][sizes_keys[i]]['acc'][j] = knn_acc\n",
    "            class_metrics['KNN'][method][sizes_keys[i]]['auc'][j] = knn_auc\n",
    "            class_metrics['KNN'][method][sizes_keys[i]]['f1'][j] = knn_f1\n",
    "\n",
    "\n",
    "\n",
    "# Calculate and store computation time of the framework\n",
    "end = time.time()\n",
    "total_time = end - start \n",
    "\n",
    "# Save results as dictionaries using pickle \n",
    "# Synthetic data metrics \n",
    "with open(\"sdg_metrics.pkl\", \"wb\") as sdg_dict:\n",
    "    pickle.dump(sdg_metrics, sdg_dict)\n",
    "sdg_dict.close()\n",
    "\n",
    "# Classification metrics \n",
    "with open(\"class_metrics.pkl\", \"wb\") as class_metrics_dict:\n",
    "    pickle.dump(class_metrics, class_metrics_dict)\n",
    "class_metrics_dict.close()\n",
    "\n",
    "# Hyperparameters \n",
    "with open(\"hyperparameters.pkl\", \"wb\") as hp_dict:\n",
    "    pickle.dump(hyperparameters, hp_dict)\n",
    "hp_dict.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIGURE I - Scatter plots with trend line: Metrics vs. Data size\n",
    "    \n",
    "# Split CTGAN and Gaussian Copula methods to plot them separately\n",
    "ctgan_combinations = [comb1, comb3, comb5, comb7]\n",
    "gc_combinations = [comb2, comb4, comb6, comb8]\n",
    "       \n",
    "# Studied metrics\n",
    "mets = [\"PCD\",\"MMD\",\"KLD\"]\n",
    "sizes = sizes_balance1\n",
    "\n",
    "# Chosen colors for each combinations\n",
    "ctgan_colors = [\"k\",\"r\",\"g\",\"b\"]\n",
    "gc_colors = [\"c\",\"m\",\"y\",\"orange\"]\n",
    "    \n",
    "# Figure \n",
    "fig, axs = plt.subplots(3,2)\n",
    "    \n",
    "# Set IEEE style \n",
    "plt.style.use(['science','ieee'])\n",
    "\n",
    "# CTGAN Plotting\n",
    "for i in range(len(ctgan_combinations)):\n",
    "\n",
    "   temp_pcd  = np.zeros(len(sizes_keys)) # variable to generate polyfit\n",
    "   temp_mmd  = np.zeros(len(sizes_keys))\n",
    "   temp_kld  = np.zeros(len(sizes_keys))\n",
    "\n",
    "   for j in range(len(sizes_keys)):\n",
    "\n",
    "      k = -1 # counter to -1 one to begin in 0\n",
    "\n",
    "      for metric in mets :\n",
    "\n",
    "         k = k + 1 # counter increments to draw the next cell\n",
    "\n",
    "         scatter1 = axs[k,0].scatter(sizes[j], sdg_metrics[ctgan_combinations[i]][sizes_keys[j]][metric].mean(), color = ctgan_colors[i])\n",
    "    \n",
    "      temp_pcd[j] = sdg_metrics[ctgan_combinations[i]][sizes_keys[j]]['PCD'].mean()\n",
    "      temp_mmd[j] = sdg_metrics[ctgan_combinations[i]][sizes_keys[j]]['MMD'].mean()\n",
    "      temp_kld[j] = sdg_metrics[ctgan_combinations[i]][sizes_keys[j]]['KLD'].mean()\n",
    "    \n",
    "   # Calulate and draw the polynom\n",
    "   z_pcd = np.polyfit(sizes, temp_pcd, 1)\n",
    "   p_pcd = np.poly1d(z_pcd)\n",
    "\n",
    "   z_mmd = np.polyfit(sizes, temp_mmd, 1)\n",
    "   p_mmd = np.poly1d(z_mmd)\n",
    "\n",
    "   z_kld = np.polyfit(sizes, temp_kld, 1)\n",
    "   p_kld = np.poly1d(z_kld)\n",
    "\n",
    "   # Line format must be specified different with orange colour\n",
    "   line = ctgan_colors[i]+\"--\"\n",
    "   axs[0,0].plot(sizes,p_pcd(sizes), line)\n",
    "   axs[1,0].plot(sizes,p_mmd(sizes), line)\n",
    "   axs[2,0].plot(sizes,p_kld(sizes), line)\n",
    "\n",
    "# Gaussian Copula Plotting\n",
    "for i in range(len(gc_combinations)):\n",
    "\n",
    "   temp_pcd  = np.zeros(len(sizes_keys)) # variable to generate polyfit\n",
    "   temp_mmd  = np.zeros(len(sizes_keys))\n",
    "   temp_kld  = np.zeros(len(sizes_keys))\n",
    "\n",
    "   for j in range(len(sizes_keys)):\n",
    "\n",
    "      k = -1 # counter to -1 one to begin in 0\n",
    "\n",
    "      for metric in mets :\n",
    "\n",
    "         k = k + 1 # counter increments to draw the next cell\n",
    "\n",
    "         scatter2 = axs[k,1].scatter(sizes[j], sdg_metrics[gc_combinations[i]][sizes_keys[j]][metric].mean(), color = gc_colors[i])\n",
    "    \n",
    "      temp_pcd[j] = sdg_metrics[gc_combinations[i]][sizes_keys[j]]['PCD'].mean()\n",
    "      temp_mmd[j] = sdg_metrics[gc_combinations[i]][sizes_keys[j]]['MMD'].mean()\n",
    "      temp_kld[j] = sdg_metrics[gc_combinations[i]][sizes_keys[j]]['KLD'].mean()\n",
    "    \n",
    "   # Calulate and draw the polynom\n",
    "   z_pcd = np.polyfit(sizes, temp_pcd, 1)\n",
    "   p_pcd = np.poly1d(z_pcd)\n",
    "\n",
    "   z_mmd = np.polyfit(sizes, temp_mmd, 1)\n",
    "   p_mmd = np.poly1d(z_mmd)\n",
    "\n",
    "   z_kld = np.polyfit(sizes, temp_kld, 1)\n",
    "   p_kld = np.poly1d(z_kld)\n",
    "\n",
    "   # Line format must be specified different with orange colour\n",
    "   axs[0,1].plot(sizes,p_pcd(sizes), c = gc_colors[i], ls = \"--\")\n",
    "   axs[1,1].plot(sizes,p_mmd(sizes), c = gc_colors[i], ls = \"--\")\n",
    "   axs[2,1].plot(sizes,p_kld(sizes), c = gc_colors[i], ls = \"--\")\n",
    "\n",
    "# Remove x-labels\n",
    "axs[0,0].set_xticklabels([])\n",
    "axs[1,0].set_xticklabels([])\n",
    "axs[0,1].set_xticklabels([])\n",
    "axs[1,1].set_xticklabels([])\n",
    "\n",
    "# Set figure text\n",
    "fig.text(0.5, 0.04, 'Nº of samples', ha='center')\n",
    "fig.text(0.02, 0.75, 'PCD', va='center', rotation='vertical')\n",
    "fig.text(0.02, 0.5, 'MMD', va='center', rotation='vertical')\n",
    "fig.text(0.02, 0.25, 'KLD', va='center', rotation='vertical')\n",
    "\n",
    "# Set legend\n",
    "axs[0,0].legend(ctgan_combinations, bbox_to_anchor=(-0.25,1.02,1,0.2), loc=\"lower left\",\n",
    "                mode=\"None\", borderaxespad=0, ncol=2, prop={'size': 4})\n",
    "axs[0,1].legend(gc_combinations, bbox_to_anchor=(0,1.02,1,0.2), loc=\"lower left\",\n",
    "                mode=\"None\", borderaxespad=0, ncol=2, prop={'size': 4})\n",
    "\n",
    "plt.savefig('Framingham_metrics_vs_synthetic_data_samples', dpi=600)\n",
    "\n",
    "# FIGURE II - F1-Score versus data samples (Best abd worst cases) \n",
    "\n",
    "best_worst = ['NC + Sep. + GC', 'NC + CTGAN'] \n",
    "\n",
    "models = ['SVM','RF', 'XGB', 'KNN']\n",
    "\n",
    "model_colors = ['b','r','k','g']\n",
    "\n",
    "fig, ax = plt.subplots(2)\n",
    "\n",
    "# Set IEEE style \n",
    "plt.style.use(['science','ieee'])\n",
    "\n",
    "# Iterating the dictionary to plot the correspondant contents   \n",
    "for m in range(len(best_worst)) :  \n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        \n",
    "        x_vector = np.zeros(len(sizes_keys)) # Vector to fill before plotting the errorbar\n",
    "        y_vector = np.zeros(len(sizes_keys))\n",
    "        err_vector = np.zeros(len(sizes_keys))\n",
    "        \n",
    "        for method in best_worst:\n",
    "            \n",
    "            for j in range(len(sizes_keys)):\n",
    "\n",
    "                x_vector[j] = sizes[j]\n",
    "                y_vector[j] = class_metrics[models[i]][best_worst[m]][sizes_keys[j]]['f1'].mean()\n",
    "                err_vector[j] = class_metrics[models[i]][best_worst[m]][sizes_keys[j]]['f1'].std()\n",
    "\n",
    "        ax[m].errorbar(x_vector, y_vector, err_vector, capsize = 4.0, linestyle=':', marker='o', color=model_colors[i])\n",
    "\n",
    "# Set figure text \n",
    "fig.text(0.5, 0.04, 'Nº of samples', ha='center')\n",
    "fig.text(0.01, 0.5, 'F1-score', va='center', rotation='vertical')\n",
    "\n",
    "# Write the name of the chosen methods\n",
    "fig.text(0.20, 0.15, best_worst[0])\n",
    "fig.text(0.20, 0.55, best_worst[1])\n",
    "\n",
    "# Remove x-labels\n",
    "ax[0].set_xticklabels([])\n",
    "\n",
    "# Set legend \n",
    "ax[0].legend(models, bbox_to_anchor=(0.07,1.02,1,0.2), loc=\"lower left\",\n",
    "                mode=\"None\", borderaxespad=0, ncol=4, prop={'size': 6})\n",
    "\n",
    "# Plot the reference lines (Validation results without synthetic data)\n",
    "ax[0].axhline(y=SVM_f1_nosynth, color='b', linestyle='--')  \n",
    "ax[0].axhline(y=rf_f1_nosynth, color='r', linestyle='--') \n",
    "ax[0].axhline(y=xgb_f1_nosynth, color='k', linestyle='--') \n",
    "ax[0].axhline(y=knn_f1_nosynth, color='g', linestyle='--')  \n",
    "ax[1].axhline(y=SVM_f1_nosynth, color='b', linestyle='--')  \n",
    "ax[1].axhline(y=rf_f1_nosynth, color='r', linestyle='--') \n",
    "ax[1].axhline(y=xgb_f1_nosynth, color='k', linestyle='--') \n",
    "ax[1].axhline(y=knn_f1_nosynth, color='g', linestyle='--')              \n",
    "\n",
    "plt.savefig('Framingham_f1_vs_data_samples', dpi = 600)\n",
    "\n",
    "# FIGURE III: Metrics vs. F1-Score\n",
    "\n",
    "# Best combination: ADASYN + GC\n",
    "best_method = \"NC + GC\"\n",
    "\n",
    "fig, ax = plt.subplots(3)\n",
    "\n",
    "plt.style.use(['science','ieee'])\n",
    "\n",
    "for i in range(len(models)): \n",
    "\n",
    "    temp_f1  = np.zeros(len(sizes_keys)) # variable to generate polyfit\n",
    "    temp_pcd  = np.zeros(len(sizes_keys)) \n",
    "    temp_mmd  = np.zeros(len(sizes_keys)) \n",
    "    temp_kld  = np.zeros(len(sizes_keys)) \n",
    "\n",
    "    for j in range(len(sizes_keys)): \n",
    "        \n",
    "        for k in range(len(mets)):\n",
    "            \n",
    "            scatter1 = ax[k].scatter(class_metrics[models[i]][best_method][sizes_keys[j]]['f1'].mean(), \n",
    "                        sdg_metrics[best_method][sizes_keys[j]][mets[k]].mean(),\n",
    "                        color = model_colors[i])            \n",
    "    \n",
    "        temp_f1[j] = class_metrics[models[i]][best_method][sizes_keys[j]]['f1'].mean()\n",
    "        temp_pcd[j] = sdg_metrics[best_method][sizes_keys[j]]['PCD'].mean()\n",
    "        temp_mmd[j] = sdg_metrics[best_method][sizes_keys[j]]['MMD'].mean()\n",
    "        temp_kld[j] = sdg_metrics[best_method][sizes_keys[j]]['KLD'].mean()\n",
    "    \n",
    "    line = model_colors[i]+\"--\"\n",
    "\n",
    "    z_pcd = np.polyfit(temp_f1, temp_pcd, 1)\n",
    "    p_pcd = np.poly1d(z_pcd)\n",
    "    ax[0].plot(temp_f1,p_pcd(temp_f1), line)\n",
    "\n",
    "    z_mmd = np.polyfit(temp_f1, temp_mmd, 1)\n",
    "    p_mmd = np.poly1d(z_mmd)\n",
    "    ax[1].plot(temp_f1,p_mmd(temp_f1), line)\n",
    "\n",
    "    z_kld = np.polyfit(temp_f1, temp_kld, 1)\n",
    "    p_kld = np.poly1d(z_kld)\n",
    "    ax[2].plot(temp_f1,p_kld(temp_f1), line)\n",
    "\n",
    "# Set figure text \n",
    "fig.text(0.5, 0.04, 'F1-Score', ha='center')\n",
    "fig.text(0.01, 0.75, 'PCD', va='center', rotation='vertical')\n",
    "fig.text(0.01, 0.5, 'MMD', va='center', rotation='vertical')\n",
    "fig.text(0.01, 0.25, 'KLD', va='center', rotation='vertical')\n",
    "\n",
    "# Remove x-labels\n",
    "ax[0].set_xticklabels([])\n",
    "ax[1].set_xticklabels([])\n",
    "\n",
    "# Set legend \n",
    "ax[0].legend(models, bbox_to_anchor=(0.07,1.02,1,0.2), loc=\"lower left\",\n",
    "                mode=\"None\", borderaxespad=0, ncol=4, prop={'size': 6})\n",
    "# Save figure\n",
    "plt.savefig('Framingham_sdg_metrics_vs_f1_score', dpi=600) "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f77607bdd0689b41f154633b23853ec31fe773c64b2ce3741f2eac487cf945d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('SDG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
